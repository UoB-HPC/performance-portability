#!/bin/bash
#SBATCH -p icelake
#SBATCH -A MACINTOSH-SMITH-SL2-CPU
#SBATCH --nodes=8
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=76
#SBATCH --distribution=block:block
#SBATCH -t 03:30:00

set -eu

cd "$RUN_DIR" || exit 2
date
echo "$PWD"
ldd "$BENCHMARK_EXE"

NP=$SLURM_JOB_NUM_NODES
DECK="$PWD/CloverLeaf/InputDecks/clover_bm${INPUT_BM}.in"
NCPUS=$(nproc)

echo "master=$(hostname) nproc=$NCPUS"
echo "mpicc=$(which mpiexec)"
echo "PWD=$PWD"
echo "NP=$NP"
echo "CONFIG=$CONFIG"
echo "BENCHMARK_EXE=$BENCHMARK_EXE"
echo "DECK=$DECK"
echo "======"

mkdir -p ../out

# gdb -batch -ex "run" -ex "bt" --args
# gdb -batch -ex "run" -ex "bt" -ex "quit" --args

case "$COMPILER" in
oneapi-*)
    module load gcc/11
    ;;
esac

case "$MODEL" in
sycl-acc)
    # With host task on, staging switches to interop_handle which is OpenCL on CPUs, and we can't handle that
    # with staging on. We revert to get_host_access which probably internally calls clEnqueueMapBuffer so
    # the bahaviour should be the same.
    opts+=" --device 1"
    opts+=" --staging-buffer true"
    ;;
sycl-usm)
    opts+=" --device 1"
    opts+=" --staging-buffer false"
    ;;
tbb | std-* | kokkos | omp)
    opts+=" --staging-buffer false"
    ;; # no-op
*)
    echo "Unknown run configuration for model '$MODEL'"
    exit 1
    ;;
esac

function create_command() {
    case "$MODEL" in
    sycl-*) echo "$2 --device 1" ;;
    tbb | std-indices)
        case "$COMPILER" in
        nvhpc-*) echo "$2" ;; # NVHPC doesn't use TBB
        *)
            if [ "$1" == "rank" ]; then
                echo "taskset -c \$MPI_LOCALRANKID $2"
            else
                echo "$2"
            fi
            ;; # TBB
        esac
        ;;
    *) echo "$2" ;; # keep
    esac
}

(
    set -o xtrace
    export OMP_NUM_THREADS=$NCPUS
    export OMP_PROC_BIND=true
    export OMP_PLACES=cores

    export DPCPP_CPU_NUM_CUS=$NCPUS
    export DPCPP_CPU_PLACES=numa_domains
    export DPCPP_CPU_CU_AFFINITY=close
    export DPCPP_CPU_SCHEDULE=static

    for i in $(seq "$NP" -1 1); do
        echo ">>> Using 1R/N $i"
        mpiexec -np "$i" -print-rank-map \
            sh -c "$(create_command node "$BENCHMARK_EXE --file $DECK --out ../out/clover_np${i}_${CONFIG}_${INPUT_BM}.out $opts")"
    done
)
(
    set -o xtrace
    export OMP_NUM_THREADS=1
    export OMP_PROC_BIND=false # let hydra handle this, hangs if set to true
    # export OMP_PLACES=cores # let hydra handle this, hangs if set to core

    export DPCPP_CPU_NUM_CUS=1
    # export DPCPP_CPU_PLACES=numa_domains
    # export DPCPP_CPU_CU_AFFINITY=close # let hydra handle this, hangs if set
    export DPCPP_CPU_SCHEDULE=static

    for i in $(seq "$((NP * NCPUS))" "-$NCPUS" "$NCPUS"); do
        echo ">>> Using 1R/T $i"
        if [ "$i" == "$((NP * NCPUS))" ]; then
            i="$((NP * NCPUS - NP))"
            echo "# adjusting rank to $i to prevent MPI_Allreduce stall"
        fi
        mpiexec -np "$i" -print-rank-map -map-by core -bind-to core \
            sh -c "$(create_command rank "$BENCHMARK_EXE --file $DECK --out ../out/clover_np${i}_${CONFIG}_${INPUT_BM}.out $opts")"
    done
)
