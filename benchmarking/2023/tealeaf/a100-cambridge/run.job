#!/bin/bash
#SBATCH -p ampere
#SBATCH -A MCINTOSH-SMITH-SL2-GPU
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=4
#SBATCH --exclusive
#SBATCH -t 00:30:00

#SBATCH --mail-user=wl14928@bristol.ac.uk
#SBATCH --mail-type=ALL

set -eu

cd "$RUN_DIR" || exit 2
date
echo "$PWD"
ldd "$BENCHMARK_EXE"

NP=$SLURM_JOB_NUM_NODES
DECK="$PWD/2d/Benchmarks/tea_bm_${INPUT_BM}.in"
PROBLEMS="$PWD/2d/tea.problems"
NCPUS=$(nproc)

echo "master=$(hostname) nproc=$NCPUS"
echo "mpicc=$(which mpiexec)"
echo "PWD=$PWD"
echo "NP=$NP"
echo "CONFIG=$CONFIG"
echo "BENCHMARK_EXE=$BENCHMARK_EXE"
echo "DECK=$DECK"
echo "======"

mkdir -p ../out

# gdb -batch -ex "run" -ex "bt" --args
# gdb -batch -ex "run" -ex "bt" -ex "quit" --args

export OMP_TARGET_OFFLOAD=MANDATORY

case "$COMPILER" in
oneapi-*)
    module load gcc/11
    export ONEAPI_DEVICE_SELECTOR="cuda:*"
    ;;
esac

case "$MODEL" in
sycl-* | std-* | kokkos | omp | cuda) ;; # no-op
*)
    echo "Unknown run configuration for model '$MODEL'"
    exit 1
    ;;
esac

function create_command() {
    # IntelMPI is $MPI_LOCALRANKID
    # OpenMPI is $OMPI_COMM_WORLD_LOCAL_RANK 
    gpu_launch_prelude='export CUDA_VISIBLE_DEVICES=$OMPI_COMM_WORLD_LOCAL_RANK && echo "# CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"'
    echo "$gpu_launch_prelude && $2"
}

(
    set -o xtrace
    export OMP_NUM_THREADS=$NCPUS
    export OMP_PROC_BIND=true
    export OMP_PLACES=cores
    for i in $(seq "$((NP * SLURM_NTASKS_PER_NODE))" -1 1); do
        echo ">>> Using 1R/N $i"
        mpiexec -np "$i" -l \
            sh -c "$(create_command node "$BENCHMARK_EXE --file $DECK --problems $PROBLEMS --out ../out/tea_np${i}_${CONFIG}_${INPUT_BM}.out")"
    done
)
